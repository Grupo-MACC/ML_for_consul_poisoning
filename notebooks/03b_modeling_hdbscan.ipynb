{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d5ee7b",
   "metadata": {},
   "source": [
    "# 03_Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec1bec",
   "metadata": {},
   "source": [
    "https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c6bc3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1ac78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taxi/miniconda3/envs/consoning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import average_precision_score, adjusted_rand_score, roc_auc_score, average_precision_score, silhouette_score\n",
    "import hdbscan\n",
    "import umap\n",
    "import joblib  \n",
    "from visualization import compare_hdbscan_params,compare_hdbscan_params_fast, plot_metrics_comparison, compute_mahalanobis_VI\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e08af",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586b72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocessed_datasets/windowed_dataset_cleaned.csv')\n",
    "X = df[df.columns.difference(['is_attack'])]  # Features\n",
    "y = df['is_attack']  # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35216e73",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022e524",
   "metadata": {},
   "source": [
    "### Selecting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df59018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "param_grid = [\n",
    "    (50, 50),\n",
    "    (10, 10),\n",
    "    (20, 20),\n",
    "    (5, 5),\n",
    "]\n",
    "\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X, y, \n",
    "    train_size=10000,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbdf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escalando datos...\n",
      "Calculando VI para Mahalanobis sobre datos escalados...\n",
      "Procesando min_cluster_size=50, min_samples=50...\n"
     ]
    }
   ],
   "source": [
    "results, fig1 = compare_hdbscan_params_fast(\n",
    "    X_sample, \n",
    "    param_grid,\n",
    "    y_true=y_sample,\n",
    "    n_cols=2\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b671419",
   "metadata": {},
   "source": [
    "- The main 2 parameters in HDBSCAN are **min_cluster_size** and **min_samples**. There are also parametros like **alpha**, but it is considered better not to mess around with this parameter. With these 2 parameters we can influence the number and size of the clusters. \n",
    "- **min_cluster_size** is the minimum number of points a cluster needs to be considered a cluster. We know other data and do not want many small clusters, so we will keep it higher than 50. **min_samples** tells how conservative the model is going to be. The more conservative it is, the more samples is going to cluster as outliers.\n",
    "- **min_cluster_size = min_samples** is the way to go for outlier detection with HDBSCAN as we make dense clusters only and the rest are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e484fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plot_metrics_comparison(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fbdd3",
   "metadata": {},
   "source": [
    "- When selecting the best hyperparameteres, we look for the ones that make correct clustering. If we look at the Silhouette visuals, we don't want a clustering split which has samples with negative Silhouette scores. that would mean that some samples are closer to the closest cluster that to their own one.\n",
    "- Semi supervised approach: We also look for the ones that have the classes correctly distributed between clusters. Ideally every sample of the same class should be in the same cluster (completeness) and no sample of the other classes should be sharing that same cluster (homogeneity).\n",
    "- Here, **min_cluster_size = min_samples = 120** matches every requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e06c98",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# HDBSCAN clustering\n",
    "# -----------------------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "hdb = Pipeline([\n",
    "    ('scaler', RobustScaler()),  # Escala a media=0, std=1\n",
    "    ('hdb', hdbscan.HDBSCAN(\n",
    "        min_cluster_size=120,#Se recomienda igualar min_samples a min_cluster_size para outlier detection\n",
    "        min_samples=120, # bajarlo significa tomar mas riesgo y clusterizar mas puntos, subirlo significa ser mas conservador y detectar mas outliers\n",
    "        cluster_selection_method='eom', # 'leaf' or 'eom' EXCESS of Mass para clusters más grandes y estables\n",
    "        metric='mahalanobis',\n",
    "        VI=compute_mahalanobis_VI(X_sample),  # <-- Usar la función para calcular VI\n",
    "    ))\n",
    "])\n",
    "\n",
    "hdb_labels = hdb.fit_predict(X_sample)\n",
    "print(\"HDBSCAN clusters:\", np.unique(hdb_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad9eaf",
   "metadata": {},
   "source": [
    "### Visualizing the results: PCA & UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Varianza explicada por componente:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"Varianza total explicada:\", pca.explained_variance_ratio_.sum())\n",
    "\n",
    "df_pca = pd.DataFrame({\n",
    "    'PCA1': X_pca[:, 0],\n",
    "    'PCA2': X_pca[:, 1],\n",
    "    'HDBSCAN_cluster': hdb_labels.astype(str),\n",
    "    'is_attack': y\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_pca,\n",
    "    x='PCA1',\n",
    "    y='PCA2',\n",
    "    color='HDBSCAN_cluster',\n",
    "    title=\"PCA (2D) – HDBSCAN clusters\",\n",
    "    width=900,\n",
    "    hover_data=['is_attack'],\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.8))\n",
    "fig.update_layout(legend_title_text='HDBSCAN Cluster')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f156ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# UMAP embedding\n",
    "# -----------------------------\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.1,\n",
    "    n_components=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_umap = reducer.fit_transform(X_sample)\n",
    "\n",
    "df_vis = df.copy()\n",
    "df_vis['UMAP1'] = X_umap[:, 0]\n",
    "df_vis['UMAP2'] = X_umap[:, 1]\n",
    "df_vis['cluster_label'] = hdb_labels\n",
    "\n",
    "df_vis['Cluster'] = df_vis['cluster_label'].map(\n",
    "    lambda lbl: 'Outlier' if lbl == -1 else f'Cluster {lbl}'\n",
    ")\n",
    "\n",
    "cluster_order = ['Outlier'] + [f'Cluster {i}' for i in sorted(df_vis[df_vis['cluster_label'] != -1]['cluster_label'].unique())]\n",
    "df_vis['Cluster'] = pd.Categorical(df_vis['Cluster'], categories=cluster_order, ordered=True)\n",
    "\n",
    "hover_cols = ['is_attack', 'n_connections'] \n",
    "\n",
    "fig = px.scatter(\n",
    "    df_vis,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='Cluster',\n",
    "    hover_data=hover_cols,  # Ahora sí: columnas del DataFrame\n",
    "    title=\"UMAP + HDBSCAN clusters\",\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.8))\n",
    "fig.update_layout(legend_title_text='HDBSCAN Clusters')\n",
    "fig.write_image(\n",
    "    \"../reports/figures/umap_hdbscan_210.png\",\n",
    "    width=1000,      # ancho en píxeles\n",
    "    height=600,      # alto en píxeles\n",
    "    scale=2          # factor de escalado (2 = ~200 DPI)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0061c",
   "metadata": {},
   "source": [
    "- The model sees a big central cluster and 3 smaller islands that look like curves.\n",
    "- As the attacks are very close to some network traffic instances the model mixes them together in the cluster 0 and outliers. (zoom bottom right corner). The HDBSCAN algorithm is based on densities. The samples in the bottom right corner are very close to eachother so that might complicate the task of clustering via densities. Making the difference for the model become a very difficult task.\n",
    "- The model also sees some outliers in the top small island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['is_attack'], hdb_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7802583",
   "metadata": {},
   "source": [
    "- The outlier management is better than with GMM but the model is not able to isolate every attack as outlier or as a cluster.\n",
    "- A dedicated model for outlir detection should be used for this type of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed6b51",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d96627",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(hdb, \"../models/hdbscan_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
